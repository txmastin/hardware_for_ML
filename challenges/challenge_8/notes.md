This code implements a simple multilayer perceptron (MLP) with two input neurons, two hidden neurons, and one output neuron, trained to learn the XOR logical function. Both the hidden layer and the output layer use the sigmoid activation function. The network is trained using backpropagation and gradient descent to minimize the mean squared error between the predicted outputs and the true XOR labels. During each training epoch, the network performs a forward pass to compute predictions, calculates the error, and then adjusts the weights and biases by propagating the error backward through the network. After sufficient training, the MLP successfully learns the nonlinear decision boundary required to model the XOR function, which is not possible with a single-layer perceptron (see notes on challenge 6).
